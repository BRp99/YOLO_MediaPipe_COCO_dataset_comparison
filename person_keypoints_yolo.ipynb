{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586dba8f-ebb4-44ac-bdcf-bd04e86337b3",
   "metadata": {},
   "source": [
    "# Imports YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba179bf-eff9-482b-bf06-ec2292d7bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a88838-567a-4c2e-a076-bf34ec90f00e",
   "metadata": {},
   "source": [
    "# Model YOLO v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acfb284-4e38-493f-9c95-b1c975aaf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-pose.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d14b7f-090b-4eff-9687-9fafd224ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder= \"./extracted_images_from_coco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72710aa4-195f-4a86-8458-ca37a6631fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_0.jpg: 448x640 1 person, 198.8ms\n",
      "Speed: 4.7ms preprocess, 198.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_1.jpg: 416x640 1 person, 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_2.jpg: 640x448 1 person, 173.2ms\n",
      "Speed: 0.0ms preprocess, 173.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_3.jpg: 448x640 1 person, 157.6ms\n",
      "Speed: 3.4ms preprocess, 157.6ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_4.jpg: 448x640 1 person, 145.0ms\n",
      "Speed: 2.0ms preprocess, 145.0ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_5.jpg: 448x640 1 person, 156.6ms\n",
      "Speed: 2.5ms preprocess, 156.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_6.jpg: 640x448 1 person, 210.8ms\n",
      "Speed: 4.0ms preprocess, 210.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_7.jpg: 448x640 1 person, 216.6ms\n",
      "Speed: 6.8ms preprocess, 216.6ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_8.jpg: 640x448 1 person, 211.3ms\n",
      "Speed: 0.0ms preprocess, 211.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 C:\\Users\\bp111\\OneDrive\\Documentos\\Code\\YOLO_MediaPipe_COCO_dataset_comparison\\extracted_images_from_coco\\image_9.jpg: 384x640 1 person, 167.9ms\n",
      "Speed: 0.0ms preprocess, 167.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "╔══════════════════════════════════════════════════════════════════════╗\n",
      "║            Model yolov8n-pose with 0.5 confidence level              ║\n",
      "╠══════════════════════════════════════════════════════════════════════╣\n",
      "║ Total images processed                                            10 ║\n",
      "║ Total inference time  (seconds)                                1.933 ║\n",
      "║ Average inference time  (seconds)                              0.193 ║\n",
      "║ Processing speed  ( images/seconds)                            5.174 ║\n",
      "╚══════════════════════════════════════════════════════════════════════╝\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "results_list = []\n",
    "N = 10\n",
    "total_processing_time_yolo = 0\n",
    "total_images_yolo = 0\n",
    "processing_times_yolo = []\n",
    "\n",
    "files = filter(lambda file:file.endswith('.jpg'), os.listdir(input_folder))\n",
    "for file in files:\n",
    "    image_file = os.path.join(input_folder, file)\n",
    "\n",
    "\n",
    "# CALCULATION FOR INFERENCE TIME\n",
    "    start_time_yolo = time.time()\n",
    "    results_yolo = model(source=image_file, show=False, conf=0.5, save=False)\n",
    "    end_time_yolo = time.time()\n",
    "    \n",
    "    processing_time_yolo = end_time_yolo - start_time_yolo\n",
    "    total_processing_time_yolo += processing_time_yolo\n",
    "    total_images_yolo += 1\n",
    "    processing_times_yolo.append(processing_time_yolo)\n",
    "    \n",
    "    results_list.append(results_yolo)\n",
    "    count += 1\n",
    "    if count >= N:\n",
    "        break\n",
    "\n",
    "\n",
    " # GET PREDICTED COORDENATES\n",
    "for i,result in enumerate(results_list):\n",
    "    for r in result:\n",
    "        #print(len(result))\n",
    "        image_name = r.path.split('\\\\')[-1]\n",
    "        #print(f\"image {i+1} ({image_name})\")\n",
    "        xyn = r.keypoints.xyn\n",
    "        for j, pose in enumerate(xyn):\n",
    "            #print(f\"pose {j+1}\")\n",
    "            for k, landmark in enumerate(pose):\n",
    "                x, y = landmark\n",
    "                #print(f\"keypoints {k+1}:\\t ({x},\\t {y})\")\n",
    "\n",
    "# CALCULATE AVERAGE FOR INFERENCE TIME\n",
    "average_processing_time_yolo = total_processing_time_yolo / total_images_yolo if total_images_yolo > 0 else 0\n",
    "\n",
    "# CALCULATE PROCESSING SPEED\n",
    "processing_speed_yolo = total_images_yolo / total_processing_time_yolo if total_processing_time_yolo > 0 else 0\n",
    "\n",
    "print(\"╔══════════════════════════════════════════════════════════════════════╗\")\n",
    "print(\"║            Model yolov8n-pose with 0.5 confidence level              ║\")\n",
    "print(\"╠══════════════════════════════════════════════════════════════════════╣\")\n",
    "print(\"║ Total images processed                                           {:>3} ║\".format(total_images_yolo))\n",
    "print(\"║ Total inference time  (seconds)                           {:>10.3f} ║\".format(total_processing_time_yolo))\n",
    "print(\"║ Average inference time  (seconds)                         {:>10.3f} ║\".format(average_processing_time_yolo))\n",
    "print(\"║ Processing speed  ( images/seconds)                       {:>10.3f} ║\".format(processing_speed_yolo))\n",
    "print(\"╚══════════════════════════════════════════════════════════════════════╝\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63b567-6628-4610-99d6-bb83b0206eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
